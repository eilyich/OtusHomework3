{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stable-baselines3[extra]\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# import math\n",
    "import random\n",
    "\n",
    "import gymnasium\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from gym.wrappers import TransformObservation\n",
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(\"CUDA доступна:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./video\", exist_ok=True)\n",
    "\n",
    "def render_mp4(videopath: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Функция для рендеринга видео в формате mp4.\n",
    "    Args:\n",
    "    videopath - Путь к файлу с видео.\n",
    "    \"\"\"\n",
    "    \n",
    "    mp4 = open(videopath, 'rb').read()\n",
    "    base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "    return f'<video width=600 controls><source src=\"data:video/mp4; base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\") # , lap_complete_percent=0.95, domain_randomize=False, continuous=False\n",
    "env = RecordVideo(env, video_folder=\"video\", name_prefix=f\"Racing\", episode_trigger=lambda ep: True)\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(n_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorNet, self).__init__()\n",
    "        # Свёрточные слои остаются без изменений\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc = nn.Linear(4096, 512)\n",
    "        # Отдельные выходные головы\n",
    "        self.steer_mean = nn.Linear(512, 1)         # для рулевого управления\n",
    "        self.acc_brake_mean = nn.Linear(512, 2)       # для акселератора и тормоза\n",
    "        # Логарифмы стандартных отклонений\n",
    "        self.steer_log_std = nn.Parameter(torch.zeros(1))\n",
    "        self.acc_brake_log_std = nn.Parameter(torch.zeros(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Прямой проход по свёрточным слоям\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # Отдельно вычисляем параметры для двух групп действий\n",
    "        steer = torch.tanh(self.steer_mean(x))\n",
    "        acc_brake = torch.sigmoid(self.acc_brake_mean(x))\n",
    "        # Объединяем в единый вектор действий: [steering, acceleration, brake]\n",
    "        mean = torch.cat([steer, acc_brake], dim=-1)\n",
    "        # Аналогично объединяем log_std для каждой группы\n",
    "        steer_std = self.steer_log_std.expand_as(steer)\n",
    "        acc_brake_std = self.acc_brake_log_std.expand_as(acc_brake)\n",
    "        log_std = torch.cat([steer_std, acc_brake_std], dim=-1)\n",
    "        return mean, log_std\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Сеть Critic для CarRacing-v3.\n",
    "        Принимает изображение (3,96,96) и выдает скалярное значение ценности (value).\n",
    "        \"\"\"\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc = nn.Linear(4096, 512)\n",
    "        self.value = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x ожидается в формате [batch_size, 3, 96, 96]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = x.reshape(x.size(0), -1)  # Используем reshape вместо view\n",
    "        x = F.relu(self.fc(x))\n",
    "        value = self.value(x)\n",
    "        return value\n",
    "\n",
    "actor_func = ActorNet().to(device)\n",
    "value_func = CriticNet().to(device)\n",
    "\n",
    "def pick_sample(s):\n",
    "    with torch.no_grad():\n",
    "        # Преобразуем входное состояние s из (96, 96, 3) в (3, 96, 96) и добавляем размер батча\n",
    "        s_transposed = np.transpose(s, (2, 0, 1))\n",
    "        s_batch = np.expand_dims(s_transposed, axis=0)\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float, device=device)\n",
    "        \n",
    "        # Получаем параметры распределения из сети Actor (предполагается, что actor_func возвращает (mean, log_std))\n",
    "        mean, log_std = actor_func(s_batch)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # Создаем нормальное распределение и сэмплируем действие\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Убираем размер батча и преобразуем в NumPy-массив\n",
    "        action_np = action.squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # Возвращаем NumPy-массив с типом float64\n",
    "        return action_np.astype(np.float64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "gamma = 0.99  # дисконтирование\n",
    "reward_records = []  # массив наград\n",
    "\n",
    "# Оптимизаторы\n",
    "opt1 = torch.optim.AdamW(value_func.parameters(), lr=25e-4) \n",
    "opt2 = torch.optim.AdamW(actor_func.parameters(), lr=25e-4)\n",
    "\n",
    "# количество циклов обучения\n",
    "num_episodes = 50\n",
    "# \n",
    "for i in tqdm(range(num_episodes)):\n",
    "    # в начале эпизода обнуляем массивы и сбрасываем среду\n",
    "    done = False\n",
    "    states = [] \n",
    "    actions = []\n",
    "    rewards = []\n",
    "    s, _ = env.reset()\n",
    "    s = s.astype(np.float32) / 255.0\n",
    "\n",
    "    # пока не достигнем конечного состояния продолжаем выполнять действия\n",
    "    while not done:\n",
    "        # добавить состояние в список состояний\n",
    "        states.append(s.tolist())\n",
    "        # по текущей политике получить действие\n",
    "        a = pick_sample(s)\n",
    "        # выполнить шаг, получить награду (r), следующее состояние (s) и флаги конечного состояния (term, trunc)\n",
    "        s, r, term, trunc, _ = env.step(a)\n",
    "        # если конечное состояние - устанавливаем флаг окончания в True\n",
    "        done = term or trunc\n",
    "        # добавляем действие и награду в соответствующие массивы\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "    # print(sum(rewards))        \n",
    "\n",
    "    #\n",
    "    # Если траектория закончилась (достигли финального состояния)\n",
    "    #\n",
    "    # формируем массив полной награды для каждого состояния\n",
    "    cum_rewards = np.zeros_like(rewards)\n",
    "    reward_len = len(rewards)\n",
    "    for j in reversed(range(reward_len)):\n",
    "        cum_rewards[j] = rewards[j] + (cum_rewards[j+1]*gamma if j+1 < reward_len else 0)\n",
    "\n",
    "    #\n",
    "    # Оптимизируем параметры сетей\n",
    "    #\n",
    "\n",
    "    # Оптимизируем value loss (Critic)\n",
    "    # Обнуляем градиенты в оптимизаторе\n",
    "    opt1.zero_grad()\n",
    "    # преобразуем состояния и суммарные награды для каждого состояния в тензор\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    states = states.permute(0, 3, 1, 2)\n",
    "    cum_rewards = torch.tensor(cum_rewards, dtype=torch.float).to(device)\n",
    "\n",
    "    # Вычисляем лосс\n",
    "    values = value_func(states)\n",
    "    values = values.squeeze(dim=1)\n",
    "    vf_loss = F.mse_loss(\n",
    "        values,\n",
    "        cum_rewards,\n",
    "        reduction=\"none\")\n",
    "    # считаем градиенты\n",
    "    vf_loss.sum().backward()\n",
    "    # делаем шаг оптимизатора\n",
    "    opt1.step()\n",
    "\n",
    "    # Оптимизируем policy loss (Actor)\n",
    "    with torch.no_grad():\n",
    "        values = value_func(states)\n",
    "\n",
    "    # Обнуляем градиенты\n",
    "    opt2.zero_grad()\n",
    "    # преобразуем к тензорам\n",
    "    # actions = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.float, device=device)\n",
    "    # считаем advantage функцию\n",
    "    # advantages = cum_rewards - values\n",
    "    advantages = cum_rewards - values.detach().squeeze(dim=1)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "\n",
    "    # Получаем параметры распределения из сети Actor (распаковываем кортеж)\n",
    "    mean, log_std = actor_func(states)\n",
    "    std = torch.exp(log_std)\n",
    "\n",
    "    # Создаем нормальное распределение по каждому измерению действия\n",
    "    dist = torch.distributions.Normal(mean, std)\n",
    "    # Вычисляем log probability для каждого действия.\n",
    "    # Если действия многомерные, суммируем лог вероятности по последнему измерению.\n",
    "    log_probs = dist.log_prob(actions_tensor).sum(dim=-1)\n",
    "    # Определяем loss: умножаем отрицательные log probabilities на advantage\n",
    "    # pi_loss = -log_probs * advantages\n",
    "\n",
    "    beta = 0.05  # коэффициент энтропийного бонуса\n",
    "    entropy = dist.entropy().sum(dim=-1)  # суммируем по действиям\n",
    "    pi_loss = -log_probs * advantages - beta * entropy\n",
    "\n",
    "\n",
    "    # Вычисляем градиенты и делаем шаг оптимизатора\n",
    "    pi_loss.sum().backward()\n",
    "    opt2.step()\n",
    "\n",
    "\n",
    "    # # считаем лосс\n",
    "    # logits, _ = actor_func(states)\n",
    "    # log_probs = -F.cross_entropy(logits, actions, reduction=\"none\")\n",
    "    # pi_loss = -log_probs * advantages\n",
    "    \n",
    "    # # считаем градиент\n",
    "    # pi_loss.sum().backward()\n",
    "    # # делаем шаг оптимизатора\n",
    "    # opt2.step()\n",
    "\n",
    "    # Выводим итоговую награду в эпизоде (max 500)    \n",
    "    reward_records.append(sum(rewards))\n",
    "\n",
    "    # if i % 100 == 0:\n",
    "    #     print(\"Run episode {} with average reward {}\".format(i, np.mean(reward_records[-100:])), end=\"\\r\")\n",
    "\n",
    "    # stop if mean reward for 100 episodes > 475.0\n",
    "    # if np.average(reward_records[-100:]) > 475.0:\n",
    "    #     break\n",
    "\n",
    "print(\"\\nDone\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "\n",
    "# Plot\n",
    "plt.plot(reward_records, label='reward')\n",
    "plt.plot(average_reward, label='average reward')\n",
    "plt.xlabel('N episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replayBuffer:\n",
    "    def __init__(self, buffer_size: int):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def add(self, item):\n",
    "        if len(self.buffer) > self._next_idx:\n",
    "            self.buffer[self._next_idx] = item\n",
    "        else:\n",
    "            self.buffer.append(item)\n",
    "        if self._next_idx == self.buffer_size - 1:\n",
    "            self._next_idx = 0\n",
    "        else:\n",
    "            self._next_idx = self._next_idx + 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = [random.randint(0, len(self.buffer) - 1) for _ in range(batch_size)]\n",
    "        states   = [self.buffer[i][0] for i in indices]\n",
    "        actions  = [self.buffer[i][1] for i in indices]\n",
    "        rewards  = [self.buffer[i][2] for i in indices]\n",
    "        n_states = [self.buffer[i][3] for i in indices]\n",
    "        dones    = [self.buffer[i][4] for i in indices]\n",
    "        return states, actions, rewards, n_states, dones\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "buffer = replayBuffer(buffer_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=512):\n",
    "        super(QNet, self).__init__()\n",
    "        # Свёрточный энкодер для изображения (вход: [3, 96, 96])\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)   # Выход примерно: [32, 23, 23]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)   # Выход примерно: [64, 10, 10]\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)   # Выход примерно: [64, 8, 8]\n",
    "        # Итоговый размер фич: 64 * 8 * 8 = 4096.\n",
    "        # После свёрточных слоёв конкатенируем с вектором действия (размер 3)\n",
    "        self.fc1 = nn.Linear(4096 + 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        # s: [batch, 3, 96, 96], a: [batch, 3]\n",
    "        x = F.relu(self.conv1(s))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1)  # [batch, 4096]\n",
    "        x = torch.cat([x, a], dim=-1)  # конкатенация с действием\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=512):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        # Свёрточный энкодер для изображения\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc = nn.Linear(4096, hidden_dim)\n",
    "        # Голова для рулевого управления ([-1, 1])\n",
    "        self.steer_mean = nn.Linear(hidden_dim, 1)\n",
    "        # Голова для акселератора и тормоза ([0, 1])\n",
    "        self.acc_brake_mean = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, s):\n",
    "        # s: [batch, 3, 96, 96]\n",
    "        x = F.relu(self.conv1(s))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        steer = torch.tanh(self.steer_mean(x))\n",
    "        acc_brake = torch.sigmoid(self.acc_brake_mean(x))\n",
    "        action = torch.cat([steer, acc_brake], dim=-1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Создание моделей и оптимизаторов\n",
    "#############################\n",
    "\n",
    "# Два критика и их целевые сети\n",
    "q_origin_model1 = QNet().to(device)  # Q_phi1\n",
    "q_origin_model2 = QNet().to(device)  # Q_phi2\n",
    "q_target_model1 = QNet().to(device)  # Q_phi1'\n",
    "q_target_model2 = QNet().to(device)  # Q_phi2'\n",
    "_ = q_target_model1.requires_grad_(False)\n",
    "_ = q_target_model2.requires_grad_(False)\n",
    "\n",
    "# Актёр и его целевая сеть\n",
    "mu_origin_model = PolicyNet().to(device)  # mu_theta\n",
    "mu_target_model = PolicyNet().to(device)    # mu_theta'\n",
    "_ = mu_target_model.requires_grad_(False)\n",
    "\n",
    "gamma = 0.99\n",
    "opt_q1 = torch.optim.AdamW(q_origin_model1.parameters(), lr=0.0005)\n",
    "opt_q2 = torch.optim.AdamW(q_origin_model2.parameters(), lr=0.0005)\n",
    "opt_mu = torch.optim.AdamW(mu_origin_model.parameters(), lr=0.0005)\n",
    "\n",
    "#############################\n",
    "# Ornstein-Uhlenbeck шум\n",
    "#############################\n",
    "\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma, theta=0.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt \\\n",
    "            + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "# Теперь размер шума соответствует размерности действия (3)\n",
    "ou_action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(3), sigma=np.ones(3)*0.1)\n",
    "\n",
    "def pick_sample(s):\n",
    "    with torch.no_grad():\n",
    "        # Преобразуем изображение из формата HWC в CHW\n",
    "        s = np.transpose(s, (2, 0, 1))\n",
    "        s_batch = np.expand_dims(s, axis=0)\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n",
    "        action_det = mu_origin_model(s_batch)\n",
    "        action_det = action_det.squeeze(0)  # [3]\n",
    "        noise = ou_action_noise()\n",
    "        action = action_det.cpu().numpy() + noise\n",
    "        # Гарантируем корректные диапазоны:\n",
    "        # Для рулевого управления: [-1, 1]\n",
    "        action[0] = np.clip(action[0], -1.0, 1.0)\n",
    "        # Для акселератора и тормоза: [0, 1]\n",
    "        action[1:] = np.clip(action[1:], 0.0, 1.0)\n",
    "        return action\n",
    "\n",
    "#############################\n",
    "# Функция оптимизации\n",
    "#############################\n",
    "\n",
    "def optimize(states, actions, rewards, next_states, dones):\n",
    "    # Преобразуем состояния: список изображений (96,96,3) -> массив [batch, 96,96,3] -> транcпонируем в [batch, 3,96,96]\n",
    "    states = np.array(states)             # [batch, H, W, C]\n",
    "    # states = states.permute(0, 3, 1, 2)\n",
    "    # states = states.transpose(0, 3, 1, 2)   # [batch, 3, H, W]\n",
    "    states = np.transpose(states, (0, 3, 1, 2))\n",
    "\n",
    "    # states = states.permute(0, 3, 1, 2)\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "\n",
    "    actions = torch.tensor(actions, dtype=torch.float).to(device)  # [batch, 3]\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float).unsqueeze(1).to(device)\n",
    "    \n",
    "    next_states = np.array(next_states)\n",
    "    next_states = next_states.transpose(0, 3, 1, 2)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float).to(device)\n",
    "\n",
    "    dones = torch.tensor(dones, dtype=torch.float).unsqueeze(1).to(device)\n",
    "\n",
    "    # Вычисление таргет-значения для критиков (используем минимум из двух целевых оценок)\n",
    "    with torch.no_grad():\n",
    "        next_actions = mu_target_model(next_states)\n",
    "        q1_next = q_target_model1(next_states, next_actions)\n",
    "        q2_next = q_target_model2(next_states, next_actions)\n",
    "        q_next = torch.min(q1_next, q2_next)\n",
    "        q_target = rewards + gamma * (1.0 - dones) * q_next\n",
    "\n",
    "    # Обновление критика 1\n",
    "    opt_q1.zero_grad()\n",
    "    q1_current = q_origin_model1(states, actions)\n",
    "    loss_q1 = F.mse_loss(q1_current, q_target)\n",
    "    loss_q1.backward()\n",
    "    opt_q1.step()\n",
    "\n",
    "    # Обновление критика 2\n",
    "    opt_q2.zero_grad()\n",
    "    q2_current = q_origin_model2(states, actions)\n",
    "    loss_q2 = F.mse_loss(q2_current, q_target)\n",
    "    loss_q2.backward()\n",
    "    opt_q2.step()\n",
    "\n",
    "    # Обновление актёра: используем q_origin_model1 для вычисления градиента\n",
    "    opt_mu.zero_grad()\n",
    "    current_actions = mu_origin_model(states)\n",
    "    # Отключаем градиенты критика на время расчёта актора\n",
    "    for p in q_origin_model1.parameters():\n",
    "        p.requires_grad = False\n",
    "    actor_loss = -q_origin_model1(states, current_actions).mean()\n",
    "    actor_loss.backward()\n",
    "    opt_mu.step()\n",
    "    for p in q_origin_model1.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Обновление целевых сетей\n",
    "#############################\n",
    "\n",
    "tau = 0.002\n",
    "def update_target():\n",
    "    for param, target_param in zip(q_origin_model1.parameters(), q_target_model1.parameters()):\n",
    "         target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    for param, target_param in zip(q_origin_model2.parameters(), q_target_model2.parameters()):\n",
    "         target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    for param, target_param in zip(mu_origin_model.parameters(), mu_target_model.parameters()):\n",
    "         target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "#############################\n",
    "# Пример цикла обучения\n",
    "#############################\n",
    "\n",
    "# Предполагается, что у вас определён replay buffer с методами add() и sample()\n",
    "buffer = replayBuffer(buffer_size=20000)\n",
    "min_buffer_size = 1000\n",
    "batch_size = 250\n",
    "\n",
    "num_episodes = 2000\n",
    "reward_records = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    # s = env.reset()   # s – изображение формата (96,96,3)\n",
    "    s, _ = env.reset()\n",
    "    s = s.astype(np.float32) / 255.0\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    "    ou_action_noise.reset()  # сброс шума в начале эпизода\n",
    "    while not done:\n",
    "        a = pick_sample(s)              # выбираем действие с шумом\n",
    "        # s_next, r, done, _ = env.step(a)\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "        s_next = s_next.astype(np.float32) / 255.0\n",
    "        done = terminated or truncated\n",
    "        buffer.add([s, a, r, s_next, float(done)])\n",
    "        cum_reward += r\n",
    "\n",
    "        # Если в буфере достаточно элементов, запускаем оптимизацию\n",
    "        if buffer.length() >= max(batch_size, min_buffer_size):\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            optimize(states, actions, rewards, next_states, dones)\n",
    "            update_target()\n",
    "\n",
    "        s = s_next\n",
    "\n",
    "    reward_records.append(cum_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "\n",
    "# Plot\n",
    "plt.plot(reward_records, label='reward')\n",
    "plt.plot(average_reward, label='average reward')\n",
    "plt.xlabel('N episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем веса актёра и критиков, а также их оптимизаторы, если нужно\n",
    "torch.save({\n",
    "    'mu_origin_model': mu_origin_model.state_dict(),\n",
    "    'q_origin_model1': q_origin_model1.state_dict(),\n",
    "    'q_origin_model2': q_origin_model2.state_dict(),\n",
    "    # Можно сохранить и целевые сети, если требуется:\n",
    "    'mu_target_model': mu_target_model.state_dict(),\n",
    "    'q_target_model1': q_target_model1.state_dict(),\n",
    "    'q_target_model2': q_target_model2.state_dict(),\n",
    "    # Если требуется – состояния оптимизаторов:\n",
    "    'opt_mu': opt_mu.state_dict(),\n",
    "    'opt_q1': opt_q1.state_dict(),\n",
    "    'opt_q2': opt_q2.state_dict()\n",
    "}, \"agent_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformObservation(env, lambda obs: obs.astype(np.float32) / 255.0, observation_space=env.observation_space)\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"CnnPolicy\", env, action_noise=action_noise, verbose=1, device=device)\n",
    "model.learn(total_timesteps=5e5, log_interval=250, progress_bar=True)\n",
    "# model.save(\"ddpg_pendulum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "n_episodes = 10  # задайте нужное число эпизодов\n",
    "rewards_lst = []\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, dones, infos = vec_env.step(action)\n",
    "        episode_rewards += rewards[0]  # если векторизованное окружение, выбирайте нужный индекс\n",
    "        vec_env.render()\n",
    "        # Если хотя бы в одном из окружений эпизод завершился, делаем reset\n",
    "        if np.any(dones):\n",
    "            done = True\n",
    "    rewards_lst.append(episode_rewards)\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
