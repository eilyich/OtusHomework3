{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA доступна: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# import math\n",
    "# import random\n",
    "\n",
    "import gymnasium\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(\"CUDA доступна:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./video\", exist_ok=True)\n",
    "\n",
    "def render_mp4(videopath: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Функция для рендеринга видео в формате mp4.\n",
    "    Args:\n",
    "    videopath - Путь к файлу с видео.\n",
    "    \"\"\"\n",
    "    \n",
    "    mp4 = open(videopath, 'rb').read()\n",
    "    base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "    return f'<video width=600 controls><source src=\"data:video/mp4; base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Desk\\Data\\Otus\\OtusHomework3\\venv\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Desk\\Data\\Otus\\OtusHomework3\\video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\") # , lap_complete_percent=0.95, domain_randomize=False, continuous=False\n",
    "env = RecordVideo(env, video_folder=\"video\", name_prefix=f\"Racing\", episode_trigger=lambda e: True)\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(n_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, num_actions=3):\n",
    "        \"\"\"\n",
    "        Сеть Actor для CarRacing-v3.\n",
    "        Принимает изображение размером (3, 96, 96) и выдает для каждого из 3-х действий\n",
    "        параметр распределения: среднее (mean). Логарифм стандартного отклонения (log_std)\n",
    "        может быть реализован как независимый обучаемый параметр.\n",
    "        \"\"\"\n",
    "        super(ActorNet, self).__init__()\n",
    "        # Свёрточные слои для извлечения признаков из изображения\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)   # Вход: (3,96,96) -> выход: (32, ? , ?)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        # Расчёт выходного размера после свёрточных слоёв:\n",
    "        # conv1: (96-8)//4 + 1 = 23, conv2: (23-4)//2 + 1 = 10, conv3: (10-3)//1 + 1 = 8.\n",
    "        # Итоговый размер фич: 64 * 8 * 8 = 4096.\n",
    "        self.fc = nn.Linear(4096, 512)\n",
    "        # Выходной слой для вычисления средних значений для каждого действия (3 параметра)\n",
    "        self.action_mean = nn.Linear(512, num_actions)\n",
    "        # Логарифм стандартного отклонения – задается как обучаемый параметр независимо от входа.\n",
    "        self.log_std = nn.Parameter(torch.zeros(num_actions))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x ожидается в формате [batch_size, 3, 96, 96]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # Преобразуем в вектор\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = x.reshape(x.size(0), -1)  # Используем reshape вместо view\n",
    "        x = F.relu(self.fc(x))\n",
    "        # mean = self.action_mean(x)\n",
    "        mean = torch.tanh(self.action_mean(x))\n",
    "        # Расширяем log_std до размера [batch_size, num_actions]\n",
    "        log_std = self.log_std.expand_as(mean)\n",
    "        return mean, log_std\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Сеть Critic для CarRacing-v3.\n",
    "        Принимает изображение (3,96,96) и выдает скалярное значение ценности (value).\n",
    "        \"\"\"\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc = nn.Linear(4096, 512)\n",
    "        self.value = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x ожидается в формате [batch_size, 3, 96, 96]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = x.reshape(x.size(0), -1)  # Используем reshape вместо view\n",
    "        x = F.relu(self.fc(x))\n",
    "        value = self.value(x)\n",
    "        return value\n",
    "\n",
    "actor_func = ActorNet().to(device)\n",
    "value_func = CriticNet().to(device)\n",
    "\n",
    "def pick_sample(s):\n",
    "    with torch.no_grad():\n",
    "        # Преобразуем входное состояние s из (96, 96, 3) в (3, 96, 96) и добавляем размер батча\n",
    "        s_transposed = np.transpose(s, (2, 0, 1))\n",
    "        s_batch = np.expand_dims(s_transposed, axis=0)\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float, device=device)\n",
    "        \n",
    "        # Получаем параметры распределения из сети Actor (предполагается, что actor_func возвращает (mean, log_std))\n",
    "        mean, log_std = actor_func(s_batch)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # Создаем нормальное распределение и сэмплируем действие\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Убираем размер батча и преобразуем в NumPy-массив\n",
    "        action_np = action.squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # Возвращаем NumPy-массив с типом float64\n",
    "        return action_np.astype(np.float64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-89.01098901098825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deari\\AppData\\Local\\Temp\\ipykernel_12808\\1137363307.py:76: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  actions_tensor = torch.tensor(actions, dtype=torch.float, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-88.63636363636292\n",
      "-82.81786941580711\n",
      "-82.63888888888845\n",
      "-83.87096774193498\n",
      "-82.69896193771582\n",
      "-83.6065573770487\n",
      "-82.93515358361726\n",
      "-81.9494584837541\n",
      "-83.10810810810763\n",
      "-83.71335504885943\n",
      "-82.9931972789111\n",
      "-83.76623376623326\n",
      "-82.01438848920822\n",
      "-83.1649831649827\n",
      "-83.44370860927107\n",
      "-83.55263157894687\n",
      "-84.47204968944044\n",
      "-82.63888888888843\n",
      "-83.44370860927104\n",
      "-81.9494584837541\n",
      "-83.22147651006664\n",
      "-83.49834983498302\n",
      "-84.98498498498442\n",
      "-81.75182481751784\n",
      "-84.37499999999947\n",
      "-82.20640569394976\n",
      "-81.06060606060568\n",
      "-85.20710059171539\n",
      "-82.20640569394976\n",
      "-83.92282958199308\n",
      "-82.87671232876667\n",
      "-83.10810810810761\n",
      "-84.17721518987288\n",
      "-82.63888888888845\n",
      "-81.61764705882314\n",
      "-82.69896193771581\n",
      "-82.39436619718265\n",
      "-82.57839721254312\n",
      "-83.44370860927104\n",
      "-82.01438848920823\n",
      "-84.1269841269836\n",
      "-81.20300751879664\n",
      "-82.07885304659456\n",
      "-83.55263157894689\n",
      "-81.81818181818142\n",
      "-83.16498316498269\n",
      "-83.76623376623327\n",
      "-84.37499999999946\n",
      "-80.8429118773943\n",
      "-83.87096774193498\n",
      "-83.87096774193498\n",
      "-83.498349834983\n",
      "-82.87671232876667\n",
      "-82.9931972789111\n",
      "-82.75862068965473\n",
      "-80.9160305343508\n",
      "-80.32786885245879\n",
      "-86.14958448753399\n",
      "-82.26950354609887\n",
      "-82.39436619718268\n",
      "-83.44370860927103\n",
      "-81.48148148148111\n",
      "-82.39436619718268\n",
      "-81.4126394052041\n",
      "-82.20640569394975\n",
      "-83.10810810810763\n",
      "-82.51748251748208\n",
      "-81.4814814814811\n",
      "-84.2271293375389\n",
      "-83.10810810810764\n",
      "-81.81818181818142\n",
      "-83.10810810810764\n",
      "-82.63888888888846\n",
      "-84.56790123456736\n",
      "-80.69498069498039\n",
      "-83.92282958199306\n",
      "-83.55263157894689\n",
      "-82.26950354609887\n",
      "-80.98859315589318\n",
      "-84.52012383900875\n",
      "-81.20300751879661\n",
      "-81.68498168498131\n",
      "-79.52218430034114\n",
      "-84.02555910543079\n",
      "-83.22147651006665\n",
      "-82.57839721254311\n",
      "-76.19047619047616\n",
      "-82.20640569394975\n",
      "-77.35849056603764\n",
      "-80.51948051948024\n",
      "-84.42367601246052\n",
      "-80.5194805194803\n",
      "-83.10810810810764\n",
      "-79.38144329896886\n",
      "-84.32601880877694\n",
      "-82.81786941580711\n",
      "-83.97435897435845\n",
      "-82.51748251748208\n",
      "-83.66013071895374\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99  # дисконтирование\n",
    "reward_records = []  # массив наград\n",
    "\n",
    "# Оптимизаторы\n",
    "opt1 = torch.optim.AdamW(value_func.parameters(), lr=25e-4) \n",
    "opt2 = torch.optim.AdamW(actor_func.parameters(), lr=25e-4)\n",
    "\n",
    "# количество циклов обучения\n",
    "num_episodes = 100\n",
    "# \n",
    "for i in range(num_episodes):\n",
    "    # в начале эпизода обнуляем массивы и сбрасываем среду\n",
    "    done = False\n",
    "    states = [] \n",
    "    actions = []\n",
    "    rewards = []\n",
    "    s, _ = env.reset()\n",
    "    s = s.astype(np.float32) / 255.0\n",
    "\n",
    "    # пока не достигнем конечного состояния продолжаем выполнять действия\n",
    "    while not done:\n",
    "        # добавить состояние в список состояний\n",
    "        states.append(s.tolist())\n",
    "        # по текущей политике получить действие\n",
    "        a = pick_sample(s)\n",
    "        # выполнить шаг, получить награду (r), следующее состояние (s) и флаги конечного состояния (term, trunc)\n",
    "        s, r, term, trunc, _ = env.step(a)\n",
    "        # если конечное состояние - устанавливаем флаг окончания в True\n",
    "        done = term or trunc\n",
    "        # добавляем действие и награду в соответствующие массивы\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "    print(sum(rewards))        \n",
    "\n",
    "    #\n",
    "    # Если траектория закончилась (достигли финального состояния)\n",
    "    #\n",
    "    # формируем массив полной награды для каждого состояния\n",
    "    cum_rewards = np.zeros_like(rewards)\n",
    "    reward_len = len(rewards)\n",
    "    for j in reversed(range(reward_len)):\n",
    "        cum_rewards[j] = rewards[j] + (cum_rewards[j+1]*gamma if j+1 < reward_len else 0)\n",
    "\n",
    "    #\n",
    "    # Оптимизируем параметры сетей\n",
    "    #\n",
    "\n",
    "    # Оптимизируем value loss (Critic)\n",
    "    # Обнуляем градиенты в оптимизаторе\n",
    "    opt1.zero_grad()\n",
    "    # преобразуем состояния и суммарные награды для каждого состояния в тензор\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    states = states.permute(0, 3, 1, 2)\n",
    "    cum_rewards = torch.tensor(cum_rewards, dtype=torch.float).to(device)\n",
    "\n",
    "    # Вычисляем лосс\n",
    "    values = value_func(states)\n",
    "    values = values.squeeze(dim=1)\n",
    "    vf_loss = F.mse_loss(\n",
    "        values,\n",
    "        cum_rewards,\n",
    "        reduction=\"none\")\n",
    "    # считаем градиенты\n",
    "    vf_loss.sum().backward()\n",
    "    # делаем шаг оптимизатора\n",
    "    opt1.step()\n",
    "\n",
    "    # Оптимизируем policy loss (Actor)\n",
    "    with torch.no_grad():\n",
    "        values = value_func(states)\n",
    "\n",
    "    # Обнуляем градиенты\n",
    "    opt2.zero_grad()\n",
    "    # преобразуем к тензорам\n",
    "    # actions = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.float, device=device)\n",
    "    # считаем advantage функцию\n",
    "    advantages = cum_rewards - values\n",
    "\n",
    "    # Получаем параметры распределения из сети Actor (распаковываем кортеж)\n",
    "    mean, log_std = actor_func(states)\n",
    "    std = torch.exp(log_std)\n",
    "\n",
    "    # Создаем нормальное распределение по каждому измерению действия\n",
    "    dist = torch.distributions.Normal(mean, std)\n",
    "    # Вычисляем log probability для каждого действия.\n",
    "    # Если действия многомерные, суммируем лог вероятности по последнему измерению.\n",
    "    log_probs = dist.log_prob(actions_tensor).sum(dim=-1)\n",
    "    # Определяем loss: умножаем отрицательные log probabilities на advantage\n",
    "    # pi_loss = -log_probs * advantages\n",
    "\n",
    "    beta = 0.01  # коэффициент энтропийного бонуса\n",
    "    entropy = dist.entropy().sum(dim=-1)  # суммируем по действиям\n",
    "    pi_loss = -log_probs * advantages - beta * entropy\n",
    "\n",
    "\n",
    "    # Вычисляем градиенты и делаем шаг оптимизатора\n",
    "    pi_loss.sum().backward()\n",
    "    opt2.step()\n",
    "\n",
    "\n",
    "    # # считаем лосс\n",
    "    # logits, _ = actor_func(states)\n",
    "    # log_probs = -F.cross_entropy(logits, actions, reduction=\"none\")\n",
    "    # pi_loss = -log_probs * advantages\n",
    "    \n",
    "    # # считаем градиент\n",
    "    # pi_loss.sum().backward()\n",
    "    # # делаем шаг оптимизатора\n",
    "    # opt2.step()\n",
    "\n",
    "    # Выводим итоговую награду в эпизоде (max 500)    \n",
    "    reward_records.append(sum(rewards))\n",
    "\n",
    "    # if i % 100 == 0:\n",
    "    #     print(\"Run episode {} with average reward {}\".format(i, np.mean(reward_records[-100:])), end=\"\\r\")\n",
    "\n",
    "    # stop if mean reward for 100 episodes > 475.0\n",
    "    # if np.average(reward_records[-100:]) > 475.0:\n",
    "    #     break\n",
    "\n",
    "print(\"\\nDone\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "\n",
    "# Plot\n",
    "plt.plot(reward_records, label='reward')\n",
    "plt.plot(average_reward, label='average reward')\n",
    "plt.xlabel('N episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
