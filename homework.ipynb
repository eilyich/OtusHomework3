{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA доступна: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "import gymnasium\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(\"CUDA доступна:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(4, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        logits = self.output(outs)\n",
    "        return logits\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(4, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        value = self.output(outs)\n",
    "        return value\n",
    "\n",
    "actor_func = ActorNet().to(device)\n",
    "value_func = ValueNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_sample(s):\n",
    "    with torch.no_grad():\n",
    "        #   --> size : (1, 4)\n",
    "        s_batch = np.expand_dims(s, axis=0)\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n",
    "        # Get logits from state\n",
    "        #   --> size : (1, 2)\n",
    "        logits = actor_func(s_batch)\n",
    "        #   --> size : (2)\n",
    "        logits = logits.squeeze(dim=0)\n",
    "        # From logits to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Pick up action's sample\n",
    "        a = torch.multinomial(probs, num_samples=1)\n",
    "        # Return\n",
    "        return a.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (9216x3 and 4x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(s\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# по текущей политике получить действие\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mpick_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# выполнить шаг, получить награду (r), следующее состояние (s) и флаги конечного состояния (term, trunc)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m s, r, term, trunc, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mpick_sample\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      5\u001b[0m s_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(s_batch, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get logits from state\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#   --> size : (1, 2)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mactor_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#   --> size : (2)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Desk\\Data\\Otus\\OtusHomework3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Desk\\Data\\Otus\\OtusHomework3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mActorNet.forward\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[1;32m----> 9\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     outs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(outs)\n\u001b[0;32m     11\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(outs)\n",
      "File \u001b[1;32mc:\\Desk\\Data\\Otus\\OtusHomework3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Desk\\Data\\Otus\\OtusHomework3\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Desk\\Data\\Otus\\OtusHomework3\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (9216x3 and 4x16)"
     ]
    }
   ],
   "source": [
    "gamma = 0.99  # дисконтирование\n",
    "env = gym.make(\"CarRacing-v3\")  # среда\n",
    "reward_records = []  # массив наград\n",
    "\n",
    "# Оптимизаторы\n",
    "opt1 = torch.optim.AdamW(value_func.parameters(), lr=0.001) \n",
    "opt2 = torch.optim.AdamW(actor_func.parameters(), lr=0.001)\n",
    "\n",
    "# количество циклов обучения\n",
    "num_episodes = 1500\n",
    "# \n",
    "for i in range(num_episodes):\n",
    "    # в начале эпизода обнуляем массивы и сбрасываем среду\n",
    "    done = False\n",
    "    states = [] \n",
    "    actions = []\n",
    "    rewards = []\n",
    "    s, _ = env.reset()\n",
    "\n",
    "    # пока не достигнем конечного состояния продолжаем выполнять действия\n",
    "    while not done:\n",
    "        # добавить состояние в список состояний\n",
    "        states.append(s.tolist())\n",
    "        # по текущей политике получить действие\n",
    "        a = pick_sample(s)\n",
    "        # выполнить шаг, получить награду (r), следующее состояние (s) и флаги конечного состояния (term, trunc)\n",
    "        s, r, term, trunc, _ = env.step(a)\n",
    "        # если конечное состояние - устанавливаем флаг окончания в True\n",
    "        done = term or trunc\n",
    "        # добавляем действие и награду в соответствующие массивы\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "    #\n",
    "    # Если траектория закончилась (достигли финального состояния)\n",
    "    #\n",
    "    # формируем массив полной награды для каждого состояния\n",
    "    cum_rewards = np.zeros_like(rewards)\n",
    "    reward_len = len(rewards)\n",
    "    for j in reversed(range(reward_len)):\n",
    "        cum_rewards[j] = rewards[j] + (cum_rewards[j+1]*gamma if j+1 < reward_len else 0)\n",
    "\n",
    "    #\n",
    "    # Оптимизируем параметры сетей\n",
    "    #\n",
    "\n",
    "    # Оптимизируем value loss (Critic)\n",
    "    # Обнуляем градиенты в оптимизаторе\n",
    "    opt1.zero_grad()\n",
    "    # преобразуем состояния и суммарные награды для каждого состояния в тензор\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    cum_rewards = torch.tensor(cum_rewards, dtype=torch.float).to(device)\n",
    "\n",
    "    # Вычисляем лосс\n",
    "    values = value_func(states)\n",
    "    values = values.squeeze(dim=1)\n",
    "    vf_loss = F.mse_loss(\n",
    "        values,\n",
    "        cum_rewards,\n",
    "        reduction=\"none\")\n",
    "    # считаем градиенты\n",
    "    vf_loss.sum().backward()\n",
    "    # делаем шаг оптимизатора\n",
    "    opt1.step()\n",
    "\n",
    "    # Оптимизируем policy loss (Actor)\n",
    "    with torch.no_grad():\n",
    "        values = value_func(states)\n",
    "\n",
    "    # Обнуляем градиенты\n",
    "    opt2.zero_grad()\n",
    "    # преобразуем к тензорам\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "    # считаем advantage функцию\n",
    "    advantages = cum_rewards - values\n",
    "\n",
    "    # считаем лосс\n",
    "    logits = actor_func(states)\n",
    "    log_probs = -F.cross_entropy(logits, actions, reduction=\"none\")\n",
    "    pi_loss = -log_probs * advantages\n",
    "    \n",
    "    # считаем градиент\n",
    "    pi_loss.sum().backward()\n",
    "    # делаем шаг оптимизатора\n",
    "    opt2.step()\n",
    "\n",
    "    # Выводим итоговую награду в эпизоде (max 500)    \n",
    "    reward_records.append(sum(rewards))\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"Run episode {} with average reward {}\".format(i, np.mean(reward_records[-100:])), end=\"\\r\")\n",
    "\n",
    "    # stop if mean reward for 100 episodes > 475.0\n",
    "    if np.average(reward_records[-100:]) > 475.0:\n",
    "        break\n",
    "\n",
    "print(\"\\nDone\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recent 50 interval average\n",
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "\n",
    "# Plot\n",
    "plt.plot(reward_records, label='reward')\n",
    "plt.plot(average_reward, label='average reward')\n",
    "plt.xlabel('N episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
